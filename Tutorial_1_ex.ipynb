{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial_1_ex.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCueQHfqopolwKqH5TltZK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bogdan-Wiederspan/cnn_notebooks/blob/main/Tutorial_1_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# History\n",
        "Machine learning is a topic that is researched for quite a long time. Biggest problem in the past was the lack of computational ressources. Especially image recognition was terrible in this department, since the analyzation of pictures needed huge feed-forward-networks (bad scaling of parameter numbers). \n",
        "\n",
        "Feed-forward-networks are a bad choice to analyze images, since the learned mapping is not translation or rotation-invariant. This means, that the network could not recognize for example a deer, when the deer was slightly tildet or not centered.\n",
        "\n",
        "LeNet-5 was one of the earliest convolutional neural networks and promoted the development of deep learning. The main benefit of convolutional networks is their invariance in translation and rotation due to the usage of kernel-stride. The other benefit of this approach is that the kernel window provides a local information source. was the reduction of learnable parameters by introduction of convultions\n",
        "\n",
        "We will recreate the LeNet-5 network to train a model on MNIST dataset!"
      ],
      "metadata": {
        "id": "sx9yzS0QFFc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def print_all_datasets():\n",
        "  \"\"\"\n",
        "  Tensorflow has plenty of datasets to play with\n",
        "  a list can be printed with this command\n",
        "  \"\"\"\n",
        "  for dataset in tfds.list_builders():\n",
        "    print(dataset)\n",
        "    \n"
      ],
      "metadata": {
        "id": "whXJ26uKlRf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The dataset\n",
        "Lets first load the MNIST dataset into our memory by utilising the ```tensorflow_datasets.load('mnist')``` function. \n",
        "\n",
        "This functions does many more things in the background. The dataset is pre-split loaded and the ```split``` argument describes what part of the split should be returned (\"train\", \"valid\", \"test\"). \n",
        "\n",
        "The keyword argument ```as_supervised``` says the function to also return the labels of the dataset. The return value is a tuple of the form: (value, label).\n",
        "\n",
        "The last interesting keyword argument in this function is ```batch_size=-1```.\n",
        "The loaded dataset contains many samples of data. Settings this keyword to -1 loads the whole dataset into the RAM. \n",
        "\n",
        "**A word of warning:** This is something one does not normally want. The main reason are again computational boundaries: like having not enough RAM. The dataset we use today is actually quiet small so this is not a problem. In your everyday life you would use the batch argument to sample a subset, a batch, of the data."
      ],
      "metadata": {
        "id": "s1_E9HDQJpfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data pre-splitted with the labels \n",
        "(train_image, train_label_ints), (test_image, test_label) = tfds.load(name='mnist' , \n",
        "                                                                 split=['train', 'test'], \n",
        "                                                                 shuffle_files=True, \n",
        "                                                                 as_supervised=True, \n",
        "                                                                 batch_size=-1, \n",
        "                                                                 data_dir = '/dataset/')\n",
        "\n",
        "# one-hot-encoding ensures that the categories are vectors and not plain ints\n",
        "# depth: number of categories \n",
        "train_label = tf.one_hot(\n",
        "            indices = train_label_ints,\n",
        "            depth = 10\n",
        ")\n",
        "\n",
        "# Lets see our one hot encoded vector\n",
        "print(train_label)"
      ],
      "metadata": {
        "id": "bWz8FiXSla7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkout the dataset\n",
        "The first step of every person handling data is plotting the data to get a feeling of its statistical distribution. A handy command to do so is the ```shape``` of the data."
      ],
      "metadata": {
        "id": "fV2sepc8siyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f' traindata shape: {?}\\n testdata shape: {?}')"
      ],
      "metadata": {
        "id": "o9sWkTkRliVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first argument of the data shape is the number of samples. MNIST training set has 60000 images, while the test has only 10000 images.\n",
        "\n",
        "The last three arguments of the shape are the pixel values of the images.\n",
        "The dataformat to stores these pixel values is a numpy array or tensor (which is a numpy array that can also live on a GPU).\n",
        "\n",
        "This means the second argument in the shape are the number of pixel values of a column. The third argument describes the number of pixels of a row. Both are 28, so the image is a square (conv-nets will always work on square pictures, of the downside of using them). The last argument is image specific and describes the color-channel. A value of 1 means that we have gray-scaled images. Imshow does not like channel values when plotting the picture so you need to remove the axis (set to 0).\n",
        "\n",
        "Lets plot the dataset to see some samples:"
      ],
      "metadata": {
        "id": "OH7VqJE-JY5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2,5)\n",
        "\n",
        "for ax, image in zip(ax.flatten(), train_image[0:10,?,?,?]):\n",
        "  # the last index (channel) needs to be 0 for gray-scaled images\n",
        "  ax.imshow(image,cmap='gray')\n",
        "# makes the subplots nicer by put in some distance between the plots\n",
        "fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pmORP3dvxZhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What kind of data does the MNIST dataset consist of ?\n",
        "\n",
        "- square pictures\n",
        "- hand drawn numbers going from 0 to 9 \n",
        "- gray scaled numbers\n",
        "\n",
        "MNIST was the dataset used by Yann LeCun (todays director of AI at Facebook) to test the very first convolutional neural networkLeNet, .\n",
        "\n",
        "We want to recreate this network and get a better feeling about Keras modelbuilding. The blueprint are depicted in the script, but for convenience reason here they are: \n",
        "![An image](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Comparison_image_neural_networks.svg/1920px-Comparison_image_neural_networks.svg.png)\n",
        "\n",
        "We see LeNet to the left and the much more advanced network AlexNet on the right. Its quiet interesting how similar, but yet so different they are. \n",
        "AlexNet is much more advanced than the plain old LeNet. \n",
        "\n",
        "Could you think of some reasons why:\n",
        "- they swapped all sigmoid functions out\n",
        "- the swapped the Pooling from average to max?\n",
        "- they use now 3 conv-layers with small window size insted of one with bigger kernel size\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xWKIEhI-2hc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general Conv-networks can be separated into 2 blocks: \n",
        "- First conv-part (filters) \n",
        "- End feed-forward-network (classficator). \n",
        "In the convolutional-part of the network filters are trained to detect different propeties. They workout the different features of the image. These processed information are then used as input of a feed-forward to classify the input."
      ],
      "metadata": {
        "id": "ETcAToxTUcK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build LeNet with Keras:\n",
        "Lets build the model. Our starting point is a ```tf.keras.models.Sequential()```object. A sequential object is a linear container holding the networks layer and have some handy functions defined to pass inputs through the layers.\n",
        "\n",
        "You can add a simply by using ```model.add(keras_layer)```\n",
        "\n",
        "A list of possible keras_layer can be found here: https://www.tensorflow.org/api_docs/python/tf/keras/layers. \n",
        "\n",
        "By looking at the Picture of LeNet we need a ***Dense Layer, a Pooling Layer*** and atleast somekind of ***ConvND*** layer (which Dimension? Remember your input data shape!). \n",
        "\n",
        "These networks have many arguments like stride or padding, or kernel size.\n",
        "You find an explanation to all arguments in the script.\n",
        "\n",
        "The most frustating part of setting up CNNs is a shape missmatch.\n",
        "This happens quiet often, since each layer is capable of reducing the output size. Use the formular given in the script about the output shape of the network!\n",
        "\n",
        "\n",
        "**Hint to the formular in the script**:\n",
        "``` padding``` can be \"same\" or \"valid\". valid is the default and means no pading is added. Meanwhile with same a padding with 0 is done.\n",
        "\n",
        "```stride``` describes the number fields that the kernel move (stride).\n",
        "The + 1 in the formular is the bias!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aPc5p_j2VTQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To help you a little in building the network the start and end is already given. Compare the code of the layer with the Image of LeNet to get a better understand what they do!\n",
        "\n",
        "Try to answer the following questions while learning how to use keras. Keras error messanges help you to understand whats happening, but also the ```model.summary()``` command\n",
        "- what happens with the output if padding is \"same\" and stride is 1 (default)\n",
        "- how does the output shape changes by using a PoolingLayer with poolsize (2,2)\n",
        "- why do we flatten the output \n",
        "- what is the shape after flatten?\n",
        "- what work is done by the dense network\n",
        "- why do we use a softmax activation function at the end?\n",
        "- which value has the sum of the softmax function output?"
      ],
      "metadata": {
        "id": "Y2czAbhVJRx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "# alias to make it shorter to access the layers\n",
        "\n",
        "layers = tf.keras.layers\n",
        "\n",
        "model.add(layers.Conv?D(filters=?, kernel_size=(5, 5), activation='sigmoid', input_shape=(?,?,1), padding=\"same\"))\n",
        "model.add(layers.?PoolingLayer(pool_size=(2,2), strides=?))\n",
        "\n",
        "?\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "?\n",
        "\n",
        "model.add(layers.Dense(units=10, activation = 'softmax'))\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "po_z7QT_Je1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Training:\n",
        "Okay we got our model. Now we need to assemble the model with an optimizer. To start the trainings process run the ```model.fit()``` command. Adjust the Hyperparameters before doing so.\n"
      ],
      "metadata": {
        "id": "mBf6mAcYb-Sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_image.shape,train_label.shape)\n"
      ],
      "metadata": {
        "id": "FFrt0DDHpIYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"Adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\", \"categorical_crossentropy\"])\n",
        "\n",
        "\n",
        "batch_size=128\n",
        "epoch=2\n",
        "shuffel=True\n",
        "subset = ?      # to create a subset of your data by slicing\n",
        "\n",
        "train_history = model.fit(train_image[0:subset],\n",
        "                          train_label[0:subset],\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=epoch,\n",
        "                          verbose=True,\n",
        "                          shuffle=shuffel)"
      ],
      "metadata": {
        "id": "gnn60u-MdUL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the model to predict an input\n",
        "test_subset = ?\n",
        "\n",
        "test_image_subset = test_image[0:test_subset]\n",
        "test_label_subset = test_label[0:test_subset]\n",
        "\n",
        "# predicts 10 classes, class with highest probability is our best pick\n",
        "prediction = model.predict(test_image_subset)\n",
        "\n",
        "# this gives us the index of the biggest prediction value\n",
        "# since our numbers are ordered from 0 to 9 these\n",
        "# are also our prediction values\n",
        "mapped_prediction = tf.math.argmax(prediction,1)\n",
        "\n",
        "# prints the predictions and true values and compare them if they were right\n",
        "for pred, true in zip(mapped_prediction, test_label_subset):\n",
        "  print(f'Prediction: {pred}, True: {true}, Guessed right: {pred == true}')\n"
      ],
      "metadata": {
        "id": "qmhf_Aiqi0QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can one improve the networks scores? You can more Conv oder Dense layers to strenghten one part of your network! More Dense layers will increase your classification power, while more Conv layers will increase your propety filter power. "
      ],
      "metadata": {
        "id": "JXGd1uf8lqBL"
      }
    }
  ]
}