{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bogdan-Wiederspan/cnn_notebooks/blob/main/Tutorial_2_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "pilWL0-CIWFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Idea of this notebook\n",
        "In this notebook you will learn a little bit about the workflow of working with convolutional networks. Therefore, you will learn how to split data and what to look for!\n",
        "You will also learn how to fine-tune a model. Often, especially with convolutional networks, it is not feasible to start from scratch. We will go throught the process of fine-tune a pre-trained network. Since industrial-pre-trained notebooks are huge, we will discuss more of the general process. In the end you will learn a little bit about image data augmentation."
      ],
      "metadata": {
        "id": "qNIOeUScDaZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The dataset\n",
        "Lets first load the CIFAR dataset into our memory by utilising the ```tensorflow_datasets.load('cifar10')``` function. "
      ],
      "metadata": {
        "id": "lxMhBqIMFzm7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whXJ26uKlRf-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# load the dataset from the tensorflow database \n",
        "(train_data,train_label), (test_data, test_label)  = tfds.load(name='cifar10' , \n",
        "                        split=['train', 'test'], \n",
        "                        shuffle_files=True, \n",
        "                        as_supervised=True, \n",
        "                        batch_size=-1)\n",
        "\n",
        "# combine the dataset to one big dataset\n",
        "# tf.concat combines the data along an axis (=0 is the sample size)\n",
        "all_data = tf.concat(?, axis=0)\n",
        "all_label_numerical = tf.concat(?, axis=0)\n",
        "\n",
        "# one-hot-encoding converts categories into vectors\n",
        "# depth: number of categories (Cifar10 has 10 categories) \n",
        "all_label_one_hot = tf.one_hot(\n",
        "            indices = all_label_numerical,\n",
        "            depth = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1_E9HDQJpfI"
      },
      "source": [
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. We combine them along the samplesize axis, because this time you should do the split yourself.\n",
        "\n",
        "Since we wanna do supervised training ```as_supervised``` is set to true, returning always a tuple-pair of (image,label).\n",
        "Again, batch_size controls the number of samples, with -1 returning all of the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV2sepc8siyY"
      },
      "source": [
        "# Checkout the dataset\n",
        "The first thing we want to do in the first place is to explore the data a little bit (honestly this is always the first thing to do). The exploration of this dataset can be quiet annoying by simply plotting the images with matplotlib. This time we use a service by the provider of the dataset:\n",
        "https://knowyourdata-tfds.withgoogle.com/#dataset=cifar10&tab=STATS&group_by=default_segment.cifar10.label.value&select=default_segment.cifar10.label.value\n",
        "\n",
        "What information can you extract about the dataset by using the website? What can you exctrat by using the Shape of the data tensor. Normally you will ask yourself things like:\n",
        "\n",
        "- which labels are present in the dataset?\n",
        "- how often are the categories presented (distribution of the data)?\n",
        "- whats the dimension of the images\n",
        "- whats the channel of the images\n",
        "- often when ones download the data as numpy array the data is a 2D array. The second argument of this dataset would be the number 3072. Do you have any idea where this data is coming from?\n",
        "\n",
        "Another thing one can ask himself is does the network learn more with HD-Images? What kind of problem will arise with HD images?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9sWkTkRliVx"
      },
      "outputs": [],
      "source": [
        "print(f' traindata shape: {all_data.?}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH7VqJE-JY5x"
      },
      "source": [
        "This time we have pictures of size 32x32 and the picture also got a color (last shape value is 3).\n",
        "## Data distribution\n",
        "Okay. This is all fine, but how do we get information about the distribution?\n",
        "The **distribution desribes the number of unique events or categories**. \n",
        "\n",
        "The tensorflow function```tf.unique_with_counts(X)``` does excatly this. X needs to be an 1D tensor, a so called flat tensor. Flat tensors have the shape: \n",
        "```(num_all_elements,)```. Be careful with unique_with_counts, since the output tensors are not sorted! They are returned in the order how they are encountered in the dataset.\n",
        "\n",
        "The return value of this function is a tuple containing: the unique values of the tensor, the index and the number of count of the class (in this order).\n",
        "\n",
        "The questions we want you to answer is:\n",
        "- how are the samples distributed?\n",
        "- what is a good-to-learn-shape for a classifier network\n",
        "- which plot form is a good choice to look at the distribution?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get unique counts with tf.unique_with_counts\n",
        "unique_y, unique_id, unique_count = tf.unique_with_counts(all_label_numerical)\n",
        "\n",
        "# mapping of label value to label\n",
        "data_label = {\n",
        "    '0':'airplane',\n",
        "    '1':'automobile',\n",
        "    '2':'bird',\n",
        "    '3':'cat',\n",
        "    '4':'deer',\n",
        "    '5':'dog',\n",
        "    '6':'frog',\n",
        "    '7':'horse',\n",
        "    '8':'ship',\n",
        "    '9':'truck'\n",
        "}\n",
        "\n",
        "# get the labels in the correct order\n",
        "unique_label = [data_label[str(num)] for num in unique_y.numpy()]\n",
        "\n",
        "# convert the tensor into numpy arrays \n",
        "unique_y, unique_count = unique_y.numpy(), unique_count.numpy()\n",
        "\n",
        "# print out in a table like fashion\n",
        "# \\t is a tab in python \\n is a line break\n",
        "print('Value,\\t unique_y,\\t\\t unique_count')\n",
        "for y, l, c in zip(unique_y, unique_label, unique_count):\n",
        "  print(f'{y},\\t {l} \\t\\t,{c}')\n",
        "\n"
      ],
      "metadata": {
        "id": "S_vNSEP4FwRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot the distribution:\n",
        "The first approach could be to print out of the number of counts with the corresponding label. A better way is actually to use ```matplotlib.pyplot``` to plot the distribution. Which kind of plot is a good choice? Think about what you actually want; A plot to count samples in a specific category. The plot will be boring to look at, but this is just a warm up!\n",
        "\n",
        "Hint about plotting tensors:\n",
        "When you want to plot a tensor X. One needs to transfrom it back to an numpy array. This is done by using ```X.numpy()```. \n",
        "This is always possible! You can also always do the reverse thing by use the numpy arrays as input for ```tf.Tensor()```.  \n"
      ],
      "metadata": {
        "id": "C-S0wMhYJ6eV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmORP3dvxZhM"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title\n",
        "\n",
        "\n",
        "# faster and better \n",
        "# to plot a histogram of all classes\n",
        "# bins is the number of categories\n",
        "plt.?(?, bins==?)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As i said this is boring to look at. Lets split the data and see how this affects the shape."
      ],
      "metadata": {
        "id": "Kvvf1PfxGIJO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWKIEhI-2hc2"
      },
      "source": [
        "# How to split data:\n",
        "\n",
        "In real-life you need to split your data by yourself. There are actually multiple ways to split the data. A great way to do this is the ```tf.data.Dataset``` class. \n",
        "We will use a more straightforward method: we will slice the ```all_data, all_label_numerical, all_label_one_hot``` to a specific index threshold. \n",
        "\n",
        "In real-life you would split the data into 3 parts: training, validation, and testing. We will just split the data into 2 parts. \n",
        "\n",
        "Hint about the calculation of the threshold:\n",
        "The threshold is just a split-factor multiplitied with number of samples. Since indices are only ints we need to convert the threshold to an int. To round a tensor use```tf.math.round(x)```."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the threshold\n",
        "train_ratio = ?\n",
        "\n",
        "number_of_sampels = all_data.shape[0]\n",
        "\n",
        "split_threshold = ?\n",
        "split_threshold = int(split_threshold)\n",
        "\n",
        "# split the data\n",
        "# first part is training\n",
        "train_hot_encode = all_label_one_hot[:?]\n",
        "train_label = all_label_numerical[:?]\n",
        "train_input = all_data[:?]\n",
        "\n",
        "# second part is test/validation\n",
        "test_hot_encode = all_label_one_hot[?:]\n",
        "test_label = all_label_numerical[?:]\n",
        "test_input = all_data[?:]\n",
        "\n",
        "# plot the data\n",
        "fig, (ax_1,ax_2) = plt.subplots(1,2)\n",
        "ax_1.hist(train_label, bins=10, label=unique_label)\n",
        "ax_1.title.set_text(f'trainings dataset, ratio: {train_ratio}')\n",
        "ax_2.hist(test_label, bins=10, label=unique_label)\n",
        "ax_2.title.set_text(f'test dataset, ratio: {1 - train_ratio}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NazBNDaGRIAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can split the data like we want, but what is a good choice for an ratio? Many people will say 80/10/10 is a nice ratio to start with. \n",
        "\n",
        "Actually there is not real rule to split the data. It actually depends on the amount of data you have.\n",
        "\n",
        "Our goal is: \n",
        "We want to have a large trainings dataset to prevent overfitting. On the other hand we want our validation set to be still representative.\n",
        "\n",
        "Representative means that the shape of our trainings and validation/test set are looking the same. In this case we know that all classes are uniform distributed (the boring plot from before). This is of course not always possible in real datasets (especially not in physics)\n",
        " test set."
      ],
      "metadata": {
        "id": "TXGPkyYGSxT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Your task: \n",
        "Play around with the split-ratios. Start with a very low split-ration (like 0.01) and increase it step-wise. \n",
        "\n",
        "Very low values would result, by chance, in an over or underrepresentation of a class. They are methods to counter this a little bit like weighting. How could you counter this?\n",
        "How does the split ration ?/?/? converges, when the samplesize increases?\n",
        "\n",
        "Always remember when splitting data: \n",
        "Your general goal is to have a big enough trainings dataset, but also a representative"
      ],
      "metadata": {
        "id": "80Ha2fgJHn7s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPc5p_j2VTQX"
      },
      "source": [
        "\n",
        "# Use a pretrained network to predict Cifar10:\n",
        "We want to classify the categories of the cifar10 dataset. Start a training from the scratch is most of the time a bad idea. There were some network architectures discussed in the presentation. Can you think which of them would be capable to predict the cifar10 dataset (judging from your experience so far)? \n",
        "\n",
        "Many industry giants and researcher made their model public (after a certain time has passed). \n",
        "\n",
        "# How to download, load pre-trained-networks\n",
        "## Back:\n",
        "\n",
        "You can download and load many pre-trained models by utilising the ```tensorflow.keras.applications``` class of keras. A list with all models can be found here: https://keras.io/api/applications/.\n",
        "\n",
        "For example you could download the VGG16 network using:\n",
        "```model = tensorflow.keras.applications.VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(128, 128, 3)))```\n",
        "\n",
        "```weights``` is the name of the weights ( in this case \"imagenet\"). this can also be a path or if None this will be random init values.\n",
        "\n",
        "```include_top``` is a boolean to remove the fully connected classification network at the end (to replace it by one of your choice)\n",
        "\n",
        "``` input_shape ``` this is the input shape of the Input Layer\n",
        "\n",
        "## Today:\n",
        "Tensorflow-hub is the new way to share trained models. Many tensorflow user upload their model and all of them can be used by using ```tensorflow_hub.load(url)``` \n",
        "\n",
        "## General workflow of pre-tuning:\n",
        "\n",
        "Utilising a pre-trained network is called transfer learning (full guide can be found here: https://keras.io/guides/transfer_learning/). Normally when doing so we need to change the input shape as well as the classifcation network. \n",
        " \n",
        "Why might you want to utilize different image dimensions and not simply scale your input images, you may ask?\n",
        "\n",
        "There are two common reasons:\n",
        "- Your input image dimensions are smaller than what the convolutional network was trained on. Increasing their size introduces too many artifacts, which will again result in bad predictions.\n",
        "- Your images are high resolution and contain small objects that are hard to detect.\n",
        "\n",
        "Also can think about reasons why we need to switch-out the classifaction network too?\n",
        "\n",
        "There is actually a limit in your choice for the input_shape: Since our convolutional network has layers that reduce the output size it your input shape needs to be big enough!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTenaX0e7WkR"
      },
      "outputs": [],
      "source": [
        "factor = 1\n",
        "\n",
        "base_model = tf.keras.applications.VGG16(weights=\"imagenet\", include_top=?,\n",
        "\tinput_tensor=tf.keras.Input(shape=(32 * factor, 32 * factor, 3)))\n",
        "\n",
        "base_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we saw before the image input is just too small for this network which is bad. So either we scale up our images (which could result in problems) or we use another network. This time we want to use a resize keras layer to scale the image up by a factor of 4.\n"
      ],
      "metadata": {
        "id": "DOE2voKEyyqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess input the same as the network was trained with\n",
        "#tf.keras.applications.vgg16.preprocess_input()\n",
        "\n",
        "# keras layer to resize our input on the fly\n",
        "resize_layer = tf.keras.layers.Resizing(\n",
        "    height=32 * factor,\n",
        "    width=32 * factor,\n",
        "    interpolation='bilinear',\n",
        "    crop_to_aspect_ratio=False)"
      ],
      "metadata": {
        "id": "ZNFIhCY18B6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to **freeze** our base_model.\n",
        "\n",
        "Freezing is a fancy term to set the ```model.trainable``` parameter to false. This means, that the learnable parameter are NOT updated anymore! This is what we want. We want to freeze the convolutional network and train a classify on top!"
      ],
      "metadata": {
        "id": "RKxZTyim8FCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# first check the trainings status of the layers:\n",
        "for l in base_model.layers:\n",
        "    print(l.name, l.trainable)\n",
        "    # set the trainings status to ?\n",
        "    l.trainable = ?\n",
        "\n",
        "    # you can also do this specificly to a certain layer:\n",
        "    # but this time we wanted to freeze the entire base_model\n",
        "    # model.layers[n].trainable = ?\n",
        "\n",
        "# lets check again\n",
        "for l in base_model.layers:\n",
        "    print(l.name, l.trainable)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5E0VpnvK4zG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first layer we need to implement into account is a pre-processing layer. The network was trained with a certain pre-processing. For example VGG16 used featurescaling with respect to the \"imagenet\" dataset. Without knowing these values the loaded network is useless.\n",
        "\n",
        "The good thing keras always comes with a pre-process-input function: ```tf.keras.applications.vgg16.preprocess_input(\n",
        "    x, data_format=None\n",
        ")```\n",
        "\n",
        "Sadly this is a function and not a layer. But we can convert this function into a layer by using a Lambda layer wrapper:\n"
      ],
      "metadata": {
        "id": "_o1pxGMTKUJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_layer = tf.keras.layers.Lambda(tf.keras.applications.vgg16.preprocess_input, input_shape=(32 * factor, 32 * factor, 3))"
      ],
      "metadata": {
        "id": "nRx9SVEtLSxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets assemble our network parts together and check if they are still partially frozen"
      ],
      "metadata": {
        "id": "g4VbSf2h8M0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assemble new network\n",
        "\n",
        "frankenstein_model = tf.keras.models.Sequential()\n",
        "frankenstein_model.add(resize_layer)\n",
        "frankenstein_model.add(preprocess_layer)\n",
        "frankenstein_model.add(base_model)\n",
        "\n",
        "\n",
        "frankenstein_model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "? # insert classification layers \n",
        "\n",
        "frankenstein_model.add(tf.keras.layers.Dense(?, activation=\"softmax\"))\n",
        "\n",
        "frankenstein_model.build(input_shape=(None, 32 * factor,32 * factor,3))\n",
        "frankenstein_model.summary()\n",
        "\n",
        "# ooops? Why is the vgg net trainable?\n",
        "# this is the problem with keras api it does \"things\" for you without asking\n",
        "# when loading a frozen model into sequential it unfreezes the layers\n",
        "print('\\n Check of the network')\n",
        "for l in frankenstein_model.layers:\n",
        "    print(l.name, l.trainable)\n",
        "    \n",
        "\n",
        "# lets set them to freeze by hand\n",
        "print('\\n Freeze part of the network')\n",
        "for l in frankenstein_model.layers[:3]:\n",
        "    print(f'Before: {l.name, l.trainable}')\n",
        "    l.trainable = False\n",
        "\n",
        "    print(f'After: {l.trainable}')\n",
        "\n",
        "\n",
        "print('\\n Check again because of paranoia with keras')\n",
        "for l in frankenstein_model.layers:\n",
        "    print(l.name, l.trainable)\n",
        "\n"
      ],
      "metadata": {
        "id": "1kpxGFPh8L-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training:\n",
        "Our little frankenstein-model is finished. Input data is automaticly sized by the Resize layer. The filters are already trained, so it is expected to be a simple trainings run. \n",
        "\n",
        "Now we need only to compile our new shiny(/ugly)-pre-tuned model and train the linear layers to classify our dataset. \n",
        "\n",
        "You will see that the results are great, since the most part of the network is already trained. (we will definitly overfit!, accuracy of 100%) \n",
        "\n",
        "```**WARNING** \n",
        "Please training this network will take some time! BE SURE TO USE A GPU! -> Runtime -> Change runtime type -> Hardware accelerator -> GPU!```"
      ],
      "metadata": {
        "id": "032zwGK0-T8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frankenstein_model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "batch_size=128\n",
        "epoch=3\n",
        "shuffle=True\n",
        "\n",
        "#print(train_input, train_label)\n",
        "frankenstein_model.fit(train_input, train_hot_encode, batch_size=batch_size, epochs=epoch, verbose=True, shuffle=shuffle)\n",
        "\n"
      ],
      "metadata": {
        "id": "JaaFRP1T-dJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXGd1uf8lqBL"
      },
      "source": [
        "# Data augmentation\n",
        "Sometimes our data capacity is quiet limited and getting more data is not always an option. Data augmentation is the poor-mans-solution to this problem.\n",
        "We try to \"modify\" our inputs a little bit on-the-fly to put noise into our network, making it harder to overfit.\n",
        "\n",
        "## Data augmentation with images\n",
        "Now the neat part, data augmentation of images is quite simple with Keras.\n",
        "We will use the ```tensorflow.keras.preprocessing.image.ImageDataGenerator```\n",
        "(https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to augment the data on the fly (within training). This reduces your memory usage but adds some additional time cost during model training. The big pro is the generator generates an infinite stream of input data!\n",
        "\n",
        "The idea of this part is to show you how to augment a datastream and to show you how it will look like. \n",
        "\n",
        "The script below will plot 3 times the same image generated by the generator.\n",
        "Each time the generator will apply the transformation on the same image.\n",
        "Look into the api to check what the different setting do! And see the result by playing around.\n",
        "\n",
        "Data augmentation is not always benefical. Can you think of why some transformations \n",
        "- would be less useful/effective then other in the context of convolutional networks?\n",
        "- how they could even be harmful?\n",
        "\n",
        "To answer the questions think always about whats your goal! You want to \n",
        "make the network learn about a rule, thus the transformation should not harm this rule."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    featurewise_center=False,\n",
        "    samplewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    samplewise_std_normalization=False,\n",
        "    zca_whitening=False,\n",
        "    zca_epsilon=1e-06,\n",
        "    rotation_range=0,\n",
        "    width_shift_range=0.0,\n",
        "    height_shift_range=0.0,\n",
        "    brightness_range=None,\n",
        "    shear_range=0.0,\n",
        "    zoom_range=0.0,\n",
        "    channel_shift_range=0.0,\n",
        "    fill_mode='nearest',\n",
        "    cval=0.0,\n",
        "    horizontal_flip=False,\n",
        "    vertical_flip=False,\n",
        "    rescale=None,\n",
        "    preprocessing_function=None,\n",
        "    data_format=None,\n",
        "    validation_split=0.0,\n",
        "    dtype=None\n",
        ")\n",
        "\n",
        "\n",
        "# calculates any statistics required to for the transformation\n",
        "# datagen is an generator\n",
        "datagen.fit(train_input)\n",
        "\n",
        "# create a python generator by using .flow\n",
        "# when iterate through the generator a batch is sampelt\n",
        "batch = datagen.flow(train_input, train_label, batch_size=1)\n",
        "\n",
        "# fetch a sample of the generator \n",
        "sample_x, sample_y = batch.next()\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(10,7))     \n",
        "for i in range(3):\n",
        "  # removes batch size \n",
        "  sample_x = tf.squeeze(sample_x)\n",
        "  # removes negative values\n",
        "  sample_x = sample_x.numpy().astype('uint8')\n",
        "  ax.flatten()[i].imshow(sample_x)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v8PP4QONiUau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you want to train with a generator object you can use simple plain ```model.fit()```.\n",
        "This command also accept generators. We just need to call the generator again with flow and redirect the batches into our fit! This would be a fit of a model with a augmented datastream!\n",
        "\n",
        "You can try to make our little frankenstein_model better by using an augmented dataset. To restart the model you need to clear the output of all the networks before! \n"
      ],
      "metadata": {
        "id": "29uqI2_Ew8JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "batch_size = 128\n",
        "\n",
        "# Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. \n",
        "steps_per_epoch = None # None is equal to = len(train_input) // batch_size\n",
        "frankenstein_model.fit(datagen.flow(train_input, train_hot_encode, batch_size=batch_size),\n",
        "        steps_per_epoch=steps_per_epoch, epochs=epochs)"
      ],
      "metadata": {
        "id": "Ni7ymihYB8Pd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tutorial_2_ex.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0QNRtptDP3uwpbJBc4DCe",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}